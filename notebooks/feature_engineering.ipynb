{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering Pipeline v2\n",
        "\n",
        "This notebook runs the full **FeatureEngineer** pipeline from `notebooks/utils/features.py` on train, test, and holdout vital-sign time series, then saves the resulting feature matrices as Parquet files in `data/`.\n",
        "\n",
        "**Outputs:** `train_features.parquet`, `test_features.parquet`, `holdout_features.parquet`\n",
        "\n",
        "---\n",
        "\n",
        "## What the pipeline does\n",
        "\n",
        "The pipeline turns raw vital-sign rows (heart rate, BP, SpO₂, respiratory rate, etc.) and patient demographics into a single feature set suitable for downstream models (e.g. LightGBM, LSTM). All **scalers and encoders are fit on train encounters only** and then applied to test/holdout to avoid leakage.\n",
        "\n",
        "When you run the pipeline, **logging** shows which feature group is currently being created (impute → lag → derivative → rolling → … → interactions → higher-order derivatives).\n",
        "\n",
        "---\n",
        "\n",
        "## Feature groups (in pipeline order)\n",
        "\n",
        "| Step | Feature group | Description |\n",
        "|------|----------------|-------------|\n",
        "| 1 | **impute_vitals** | Fill missing vitals: neighbour interpolation, then encounter median fallback |\n",
        "| 2 | **lag** | Per-vital lags 1..`n_lags` + `warmup_progress` (0→1 over first `n_lags` rows) |\n",
        "| 3 | **derivative** | Delta (vs lag_n), delta_1s (vs lag_1), acceleration per vital |\n",
        "| 4 | **rolling_stats** | Mean, std, min, max over the lag window (e.g. last 12 steps) per vital |\n",
        "| 5 | **multiscale_rolling** | Same stats over 4 windows: 6, 12, 24, 60 steps (~30s–5 min) per vital |\n",
        "| 6 | **derived_vitals** | Pulse pressure, MAP, shock_index (HR/SBP), hr_rr_ratio (HR/RR), and their deltas |\n",
        "| 7 | **temporal** | Minutes into encounter, hour sin/cos, day-of-week sin/cos |\n",
        "| 8 | **ecg** | Per-encounter ECG: basic stats + FFT (dom_freq, LF/HF power, spectral entropy), hr_ecg_diff (if ECG data exists) |\n",
        "| 9 | **prior_label** | *(Optional)* prior_label, max_label_last_60s, ever_deteriorated — **disabled by default** to avoid label leakage on holdout |\n",
        "| 10 | **patient** | Demographics (age, BMI, pain_score; scaled), missingness flags, is_elderly/is_child, gender/marital/race/ethnicity/encounter_description (OHE), reason_risk_tier, comorbidity flags from free-text, on_cardiac_meds / on_insulin |\n",
        "| 11 | **clinical_alert** | Binary flags: tachycardia, bradycardia, hypotension, hypertension, SpO₂ low/critical, tachypnea/bradypnea, n_active_alerts |\n",
        "| 12 | **interaction** | Vital × elderly, vital × child, vital × comorbidity_count, HR × cardiac_meds, shock_index × reason_risk_tier |\n",
        "| 13 | **higher_order_derivatives** | Jerk (3rd derivative) per vital for sudden change detection |\n",
        "\n",
        "---\n",
        "\n",
        "## EDA-informed choices\n",
        "\n",
        "- **Missingness as signal:** `bmi_missing`, `pain_score_missing`, `reason_missing` — EDA showed missingness correlates with outcome.\n",
        "- **Comorbidities:** Parsed from `previous_medical_history` (hypertension, diabetes, kidney, cardiac, anemia, obesity); comorbidity count is strongly associated with deterioration.\n",
        "- **Reason-for-visit risk tier:** High (e.g. MI, stroke, gunshot) vs medium vs low — high-risk reasons have much higher label-3 rates.\n",
        "- **Encounter description:** One-hot encoded; e.g. obstetric vs ED has very different outcome rates.\n",
        "- **Medication flags:** `on_cardiac_meds` (metoprolol, nitroglycerin, etc.) strongly predicts deterioration.\n",
        "- **Dropped:** `known_allergies`, `previous_medications` (too sparse / no association); BMI/pain_score kept with missingness flags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "Add `notebooks/` to the path and load `FeatureEngineer` from `utils/features.py`. Logging is configured so the pipeline prints which feature group it is creating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from utils.features import FeatureEngineer\n",
        "\n",
        "# Show pipeline progress: which feature group is currently being created\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
        "\n",
        "DATA_DIR = Path(\"../data\")\n",
        "\n",
        "engineer = FeatureEngineer(\n",
        "    n_lags=12,\n",
        "    rolling_windows=[6, 12, 24, 60],\n",
        "    data_dir=DATA_DIR,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load raw data and run pipeline\n",
        "\n",
        "Load CSV inputs, then call `engineer.transform(...)`. Progress is logged so you can see each feature group as it is created. Use `include_prior_labels=False` (default) to avoid label leakage on holdout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating features: impute_vitals (neighbour + median fallback)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train   : (2109600, 8)  |  encounters: 2,930\n",
            "test    : (451440, 8)  |  encounters: 627\n",
            "holdout : (452880, 7)  |  encounters: 629\n",
            "\n",
            "patients: (4186, 17)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating features: lag (vital lags 1..n + warmup_progress)\n",
            "Creating features: derivative (delta, delta_1s, accel per vital)\n",
            "Creating features: rolling_stats (mean/std/min/max over lag window)\n",
            "Creating features: multiscale_rolling (mean/std/min/max per window)\n",
            "Creating features: derived_vitals (pulse_pressure, map, shock_index, hr_rr_ratio)\n",
            "Creating features: temporal (minutes_into_encounter, hour/dow sin/cos)\n",
            "Creating features: ecg (stats + FFT: dom_freq, LF/HF, spectral_entropy, hr_ecg_diff)\n",
            "Creating features: patient (demographics, comorbidity, risk tier, OHE)\n",
            "Creating features: clinical_alert (tachycardia, hypotension, spo2_low, n_active_alerts)\n",
            "Creating features: interaction (vital x elderly/child/comorbidity/cardiac_meds)\n",
            "Creating features: higher_order_derivatives (jerk per vital)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Feature pipeline complete: 283 columns\n"
          ]
        }
      ],
      "source": [
        "train_raw = pd.read_csv(DATA_DIR / \"train_data.csv\", parse_dates=[\"timestamp\"])\n",
        "test_raw = pd.read_csv(DATA_DIR / \"test_data.csv\", parse_dates=[\"timestamp\"])\n",
        "holdout_raw = pd.read_csv(DATA_DIR / \"holdout_data.csv\", parse_dates=[\"timestamp\"])\n",
        "patients = pd.read_csv(DATA_DIR / \"patients.csv\")\n",
        "\n",
        "for name, df in [(\"train\", train_raw), (\"test\", test_raw), (\"holdout\", holdout_raw)]:\n",
        "    print(f\"{name:8s}: {df.shape}  |  encounters: {df['encounter_id'].nunique():,}\")\n",
        "print(f\"\\npatients: {patients.shape}\")\n",
        "\n",
        "# include_prior_labels=False prevents label leakage on holdout\n",
        "train, test, holdout, feature_cols = engineer.transform(\n",
        "    train_raw, test_raw, holdout_raw, patients,\n",
        "    include_prior_labels=False,\n",
        ")\n",
        "print(f\"\\nFeature pipeline complete: {len(feature_cols)} columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Validation\n",
        "\n",
        "Check that the resulting feature matrices have no missing values in the feature columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train   : shape=(2109600, 286)  missing_feature_values=0\n",
            "test    : shape=(451440, 286)  missing_feature_values=0\n",
            "holdout : shape=(452880, 285)  missing_feature_values=0\n"
          ]
        }
      ],
      "source": [
        "for name, df in [(\"train\", train), (\"test\", test), (\"holdout\", holdout)]:\n",
        "    n_missing = df[feature_cols].isna().sum().sum()\n",
        "    assert n_missing == 0, f\"{name}: {n_missing} NaNs in features\"\n",
        "    print(f\"{name:8s}: shape={df.shape}  missing_feature_values={n_missing}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Save\n",
        "\n",
        "Write train/test/holdout feature DataFrames to Parquet in `data/` (using fastparquet to avoid pyarrow compatibility issues)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pickle dump the feature_cols\n",
        "import pickle\n",
        "with open(DATA_DIR / \"feature_cols.pkl\", \"wb\") as f:\n",
        "    pickle.dump(feature_cols, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved train_features.parquet, test_features.parquet, holdout_features.parquet\n"
          ]
        }
      ],
      "source": [
        "# Work around fastparquet + pandas Arrow string dtype: convert encounter_id to object\n",
        "# so fastparquet can encode it (ArrowExtensionArray raises on copy=False)\n",
        "def _to_parquet_safe(df, path):\n",
        "    df = df.copy()\n",
        "    if \"encounter_id\" in df.columns:\n",
        "        df[\"encounter_id\"] = df[\"encounter_id\"].astype(object)\n",
        "    df.to_parquet(path, index=False, engine=\"fastparquet\")\n",
        "\n",
        "_to_parquet_safe(train, DATA_DIR / \"train_features.parquet\")\n",
        "_to_parquet_safe(test, DATA_DIR / \"test_features.parquet\")\n",
        "_to_parquet_safe(holdout, DATA_DIR / \"holdout_features.parquet\")\n",
        "print(\"Saved train_features.parquet, test_features.parquet, holdout_features.parquet\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
