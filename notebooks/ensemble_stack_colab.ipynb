{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stacking Ensemble — LightGBM + XGBoost + 1D-CNN + CatBoost\n",
        "\n",
        "Combines gradient boosting and deep learning via two-level stacking:\n",
        "- **Level 0**: 4 diverse base models trained with encounter-level 5-fold CV\n",
        "- **Level 1**: Logistic regression meta-learner on stacked OOF probabilities\n",
        "\n",
        "Diversity comes from:\n",
        "1. **LightGBM** — fast GBDT with leaf-wise growth\n",
        "2. **XGBoost** — GBDT with level-wise growth (different inductive bias)\n",
        "3. **1D-CNN Fusion** — convolutional model on vital sequences + static MLP\n",
        "4. **CatBoost** — ordered boosting, symmetric trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q lightgbm xgboost catboost\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# DATA_DIR: point to your Google Drive folder containing train_features.pkl / .csv etc.\n",
        "# Upload outputs from feature_engineering_v2.ipynb (run locally or in Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from pathlib import Path\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Point to your Drive folder with train_features.pkl/csv, test_features.*, holdout_features.*
DATA_DIR = Path('/content/drive/MyDrive/data')  # or Medhack_data, etc.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "N_FOLDS = 5\n",
        "SEED = 42\n",
        "np.random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_features(path: Path, name: str):\n",
        "    \"\"\"Load from .pkl if present (faster), else .csv.\"\"\"\n",
        "    pkl_path = path / f'{name}_features.pkl'\n",
        "    csv_path = path / f'{name}_features.csv'\n",
        "    if pkl_path.exists():\n",
        "        return pd.read_pickle(pkl_path)\n",
        "    if csv_path.exists():\n",
        "        return pd.read_csv(csv_path, parse_dates=['timestamp'])\n",
        "    raise FileNotFoundError(f\"Neither {pkl_path} nor {csv_path} found. Upload feature outputs from feature_engineering_v2.\")\n",
        "\n",
        "print(\"Loading data...\")\n",
        "train_df = load_features(DATA_DIR, 'train')\n",
        "test_df = load_features(DATA_DIR, 'test')\n",
        "holdout_df = load_features(DATA_DIR, 'holdout')\n",
        "\n",
        "# Combine train + test for stacking CV (all labeled data)\n",
        "combined_df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "del train_df, test_df\n",
        "gc.collect()\n",
        "\n",
        "for name, df in [('combined', combined_df), ('holdout', holdout_df)]:\n",
        "    print(f\"{name:10s}: {df.shape}\")\n",
        "\n",
        "VITAL_COLS = ['heart_rate', 'systolic_bp', 'diastolic_bp', 'respiratory_rate', 'oxygen_saturation']\n",
        "META_COLS = ['timestamp', 'encounter_id']\n",
        "TARGET = 'label'\n",
        "PRIOR_LABEL_COLS = ['prior_label', 'max_label_last_60s', 'max_label_encounter', 'ever_deteriorated']\n",
        "USE_PRIOR_LABELS = False\n",
        "\n",
        "# Flat feature columns (for tree models) — align with feature_engineering_v2 output\n",
        "flat_feature_cols = [c for c in combined_df.columns if c not in META_COLS + [TARGET]]\n",
        "if not USE_PRIOR_LABELS:\n",
        "    flat_feature_cols = [c for c in flat_feature_cols if c not in PRIOR_LABEL_COLS]\n",
        "\n",
        "# Sequence columns for CNN: [lag12..lag1, current] per vital — matches N_LAGS=12 in feature_engineering\n",
        "N_LAGS = 12\n",
        "seq_cols = []\n",
        "for v in VITAL_COLS:\n",
        "    seq_cols.extend([f'{v}_lag{i}' for i in range(N_LAGS, 0, -1)] + [v])\n",
        "seq_cols = [c for c in seq_cols if c in combined_df.columns]\n",
        "\n",
        "# Static columns for CNN (flat minus sequence)\n",
        "static_cols = [c for c in flat_feature_cols if c not in seq_cols]\n",
        "\n",
        "print(f\"\\nFlat features: {len(flat_feature_cols)}\")\n",
        "print(f\"Sequence cols: {len(seq_cols)} (for CNN)\")\n",
        "print(f\"Static cols: {len(static_cols)} (for CNN)\")\n",
        "\n",
        "# Arrays\n",
        "X_flat = combined_df[flat_feature_cols].values.astype(np.float32)\n",
        "y = combined_df[TARGET].values.astype(int)\n",
        "groups = combined_df['encounter_id'].values\n",
        "\n",
        "X_holdout_flat = holdout_df[flat_feature_cols].values.astype(np.float32)\n",
        "\n",
        "# Class weights\n",
        "classes = np.unique(y)\n",
        "cw = compute_class_weight('balanced', classes=classes, y=y)\n",
        "class_weight_dict = dict(zip(classes, cw))\n",
        "print(f\"\\nClass weights: {class_weight_dict}\")\n",
        "print(f\"Label distribution:\\n{pd.Series(y).value_counts().sort_index()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encounter-Level K-Fold Split\n",
        "\n",
        "GroupKFold ensures no encounter appears in both train and validation within a fold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gkf = GroupKFold(n_splits=N_FOLDS)\n",
        "folds = list(gkf.split(X_flat, y, groups=groups))\n",
        "\n",
        "for i, (train_idx, val_idx) in enumerate(folds):\n",
        "    n_enc_train = len(set(groups[train_idx]))\n",
        "    n_enc_val = len(set(groups[val_idx]))\n",
        "    print(f\"Fold {i}: train={len(train_idx):>9,} ({n_enc_train} enc) | val={len(val_idx):>9,} ({n_enc_val} enc)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Base Model 1: LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lgbm_params = {\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': 4,\n",
        "    'metric': 'multi_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 127,\n",
        "    'max_depth': 10,\n",
        "    'min_child_samples': 50,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'reg_alpha': 0.1,\n",
        "    'reg_lambda': 1.0,\n",
        "    'verbosity': -1,\n",
        "    'n_jobs': -1,\n",
        "    'seed': SEED,\n",
        "}\n",
        "\n",
        "oof_lgbm = np.zeros((len(y), 4), dtype=np.float32)\n",
        "holdout_lgbm = np.zeros((len(X_holdout_flat), 4), dtype=np.float32)\n",
        "\n",
        "for fold_i, (train_idx, val_idx) in enumerate(folds):\n",
        "    print(f\"\\nFold {fold_i}...\")\n",
        "    sw_train = np.array([class_weight_dict[yi] for yi in y[train_idx]])\n",
        "    sw_val = np.array([class_weight_dict[yi] for yi in y[val_idx]])\n",
        "\n",
        "    dtrain = lgb.Dataset(X_flat[train_idx], y[train_idx], weight=sw_train, free_raw_data=False)\n",
        "    dval = lgb.Dataset(X_flat[val_idx], y[val_idx], weight=sw_val, reference=dtrain, free_raw_data=False)\n",
        "\n",
        "    model = lgb.train(\n",
        "        lgbm_params, dtrain,\n",
        "        num_boost_round=1500,\n",
        "        valid_sets=[dval],\n",
        "        callbacks=[lgb.early_stopping(50, verbose=False), lgb.log_evaluation(200)],\n",
        "    )\n",
        "\n",
        "    oof_lgbm[val_idx] = model.predict(X_flat[val_idx])\n",
        "    holdout_lgbm += model.predict(X_holdout_flat) / N_FOLDS\n",
        "\n",
        "    f1 = f1_score(y[val_idx], oof_lgbm[val_idx].argmax(1), average='macro')\n",
        "    print(f\"  Fold {fold_i} macro F1: {f1:.4f}\")\n",
        "\n",
        "oof_f1 = f1_score(y, oof_lgbm.argmax(1), average='macro')\n",
        "print(f\"\\nLightGBM OOF macro F1: {oof_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Base Model 2: XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_params = {\n",
        "    'objective': 'multi:softprob',\n",
        "    'num_class': 4,\n",
        "    'eval_metric': 'mlogloss',\n",
        "    'learning_rate': 0.05,\n",
        "    'max_depth': 8,\n",
        "    'min_child_weight': 50,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'reg_alpha': 0.1,\n",
        "    'reg_lambda': 1.0,\n",
        "    'tree_method': 'hist',\n",
        "    'seed': SEED,\n",
        "    'nthread': -1,\n",
        "}\n",
        "\n",
        "oof_xgb = np.zeros((len(y), 4), dtype=np.float32)\n",
        "holdout_xgb = np.zeros((len(X_holdout_flat), 4), dtype=np.float32)\n",
        "\n",
        "for fold_i, (train_idx, val_idx) in enumerate(folds):\n",
        "    print(f\"\\nFold {fold_i}...\")\n",
        "    sw_train = np.array([class_weight_dict[yi] for yi in y[train_idx]])\n",
        "    sw_val = np.array([class_weight_dict[yi] for yi in y[val_idx]])\n",
        "\n",
        "    dtrain = xgb.DMatrix(X_flat[train_idx], y[train_idx], weight=sw_train)\n",
        "    dval = xgb.DMatrix(X_flat[val_idx], y[val_idx], weight=sw_val)\n",
        "\n",
        "    model = xgb.train(\n",
        "        xgb_params, dtrain,\n",
        "        num_boost_round=1500,\n",
        "        evals=[(dval, 'val')],\n",
        "        early_stopping_rounds=50,\n",
        "        verbose_eval=200,\n",
        "    )\n",
        "\n",
        "    oof_xgb[val_idx] = model.predict(dval)\n",
        "    dholdout = xgb.DMatrix(X_holdout_flat)\n",
        "    holdout_xgb += model.predict(dholdout) / N_FOLDS\n",
        "\n",
        "    f1 = f1_score(y[val_idx], oof_xgb[val_idx].argmax(1), average='macro')\n",
        "    print(f\"  Fold {fold_i} macro F1: {f1:.4f}\")\n",
        "\n",
        "oof_f1 = f1_score(y, oof_xgb.argmax(1), average='macro')\n",
        "print(f\"\\nXGBoost OOF macro F1: {oof_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Base Model 3: 1D-CNN + Static Fusion\n",
        "\n",
        "Convolutional model on (13, 5) vital sequences with a static feature MLP branch.\n",
        "CNN captures local patterns (sudden spikes, drops) more efficiently than LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VitalDataset(Dataset):\n",
        "    def __init__(self, X_seq, X_static, y=None):\n",
        "        self.X_seq = torch.FloatTensor(X_seq)\n",
        "        self.X_static = torch.FloatTensor(X_static)\n",
        "        self.y = torch.LongTensor(y) if y is not None else None\n",
        "    def __len__(self):\n",
        "        return len(self.X_seq)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is not None:\n",
        "            return self.X_seq[idx], self.X_static[idx], self.y[idx]\n",
        "        return self.X_seq[idx], self.X_static[idx]\n",
        "\n",
        "\n",
        "class CNNFusionModel(nn.Module):\n",
        "    def __init__(self, n_channels=5, n_static=89, cnn_filters=64, kernel_size=3,\n",
        "                 static_hidden=64, fusion_hidden=128, dropout=0.3, num_classes=4):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(n_channels, cnn_filters, kernel_size, padding=kernel_size//2),\n",
        "            nn.BatchNorm1d(cnn_filters),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv1d(cnn_filters, cnn_filters * 2, kernel_size, padding=kernel_size//2),\n",
        "            nn.BatchNorm1d(cnn_filters * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "        )\n",
        "        cnn_out = cnn_filters * 2\n",
        "\n",
        "        self.static_net = nn.Sequential(\n",
        "            nn.Linear(n_static, static_hidden),\n",
        "            nn.BatchNorm1d(static_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(static_hidden, static_hidden // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        static_out = static_hidden // 2\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(cnn_out + static_out, fusion_hidden),\n",
        "            nn.BatchNorm1d(fusion_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(fusion_hidden, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_seq, x_static):\n",
        "        cnn_in = x_seq.transpose(1, 2)  # (batch, 13, 5) -> (batch, 5, 13)\n",
        "        cnn_out = self.cnn(cnn_in).squeeze(-1)\n",
        "        static_out = self.static_net(x_static)\n",
        "        fused = torch.cat([cnn_out, static_out], dim=1)\n",
        "        return self.classifier(fused)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_sequence(df, vital_cols, n_lags=12):\n",
        "    channels = []\n",
        "    for v in vital_cols:\n",
        "        cols = [f'{v}_lag{i}' for i in range(n_lags, 0, -1)] + [v]\n",
        "        channels.append(df[cols].values)\n",
        "    return np.stack(channels, axis=-1).astype(np.float32)\n",
        "\n",
        "X_seq = extract_sequence(combined_df, VITAL_COLS)\n",
        "X_static = combined_df[static_cols].values.astype(np.float32)\n",
        "X_seq_holdout = extract_sequence(holdout_df, VITAL_COLS)\n",
        "X_static_holdout = holdout_df[static_cols].values.astype(np.float32)\n",
        "\n",
        "# Scale (fit on all combined data since we're doing CV, not train/test split)\n",
        "seq_scaler = StandardScaler()\n",
        "n_total, n_steps, n_ch = X_seq.shape\n",
        "seq_scaler.fit(X_seq.reshape(-1, n_ch))\n",
        "X_seq = seq_scaler.transform(X_seq.reshape(-1, n_ch)).reshape(n_total, n_steps, n_ch)\n",
        "X_seq_holdout = seq_scaler.transform(X_seq_holdout.reshape(-1, n_ch)).reshape(X_seq_holdout.shape)\n",
        "\n",
        "static_scaler = StandardScaler()\n",
        "static_scaler.fit(X_static)\n",
        "X_static = static_scaler.transform(X_static)\n",
        "X_static_holdout = static_scaler.transform(X_static_holdout)\n",
        "\n",
        "print(f\"Sequence: {X_seq.shape}, Static: {X_static.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CNN_EPOCHS = 15\n",
        "CNN_BATCH = 2048\n",
        "CNN_LR = 1e-3\n",
        "\n",
        "class_weights_t = torch.FloatTensor(cw).to(device)\n",
        "\n",
        "oof_cnn = np.zeros((len(y), 4), dtype=np.float32)\n",
        "holdout_cnn = np.zeros((len(X_seq_holdout), 4), dtype=np.float32)\n",
        "\n",
        "for fold_i, (train_idx, val_idx) in enumerate(folds):\n",
        "    print(f\"\\nFold {fold_i}...\")\n",
        "\n",
        "    train_ds = VitalDataset(X_seq[train_idx], X_static[train_idx], y[train_idx])\n",
        "    val_ds = VitalDataset(X_seq[val_idx], X_static[val_idx], y[val_idx])\n",
        "    train_loader = DataLoader(train_ds, batch_size=CNN_BATCH, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=4096, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    model = CNNFusionModel(n_channels=5, n_static=len(static_cols)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights_t)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=CNN_LR, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CNN_EPOCHS)\n",
        "\n",
        "    best_f1, best_state = 0, None\n",
        "    for epoch in range(CNN_EPOCHS):\n",
        "        model.train()\n",
        "        for xseq, xstat, yb in train_loader:\n",
        "            xseq, xstat, yb = xseq.to(device), xstat.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(xseq, xstat), yb)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "        preds_list = []\n",
        "        with torch.no_grad():\n",
        "            for xseq, xstat, yb in val_loader:\n",
        "                xseq, xstat = xseq.to(device), xstat.to(device)\n",
        "                preds_list.append(torch.softmax(model(xseq, xstat), 1).cpu().numpy())\n",
        "        val_probs = np.concatenate(preds_list)\n",
        "        val_f1 = f1_score(y[val_idx], val_probs.argmax(1), average='macro')\n",
        "\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"  Epoch {epoch+1} | val F1: {val_f1:.4f}\")\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    model.eval()\n",
        "\n",
        "    # OOF predictions\n",
        "    with torch.no_grad():\n",
        "        preds_list = []\n",
        "        for xseq, xstat, _ in val_loader:\n",
        "            xseq, xstat = xseq.to(device), xstat.to(device)\n",
        "            preds_list.append(torch.softmax(model(xseq, xstat), 1).cpu().numpy())\n",
        "        oof_cnn[val_idx] = np.concatenate(preds_list)\n",
        "\n",
        "    # Holdout predictions\n",
        "    holdout_ds = VitalDataset(X_seq_holdout, X_static_holdout)\n",
        "    holdout_loader = DataLoader(holdout_ds, batch_size=4096, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    with torch.no_grad():\n",
        "        preds_list = []\n",
        "        for xseq, xstat in holdout_loader:\n",
        "            xseq, xstat = xseq.to(device), xstat.to(device)\n",
        "            preds_list.append(torch.softmax(model(xseq, xstat), 1).cpu().numpy())\n",
        "        holdout_cnn += np.concatenate(preds_list) / N_FOLDS\n",
        "\n",
        "    print(f\"  Fold {fold_i} best macro F1: {best_f1:.4f}\")\n",
        "\n",
        "oof_f1 = f1_score(y, oof_cnn.argmax(1), average='macro')\n",
        "print(f\"\\n1D-CNN OOF macro F1: {oof_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Base Model 4: CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "oof_cat = np.zeros((len(y), 4), dtype=np.float32)\n",
        "holdout_cat = np.zeros((len(X_holdout_flat), 4), dtype=np.float32)\n",
        "\n",
        "for fold_i, (train_idx, val_idx) in enumerate(folds):\n",
        "    print(f\"\\nFold {fold_i}...\")\n",
        "    sw_train = np.array([class_weight_dict[yi] for yi in y[train_idx]])\n",
        "\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=1500,\n",
        "        learning_rate=0.05,\n",
        "        depth=8,\n",
        "        l2_leaf_reg=3,\n",
        "        random_seed=SEED,\n",
        "        verbose=200,\n",
        "        task_type='GPU',\n",
        "        eval_metric='TotalF1:average=Macro',\n",
        "        early_stopping_rounds=50,\n",
        "        class_weights=class_weight_dict,\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_flat[train_idx], y[train_idx],\n",
        "        eval_set=(X_flat[val_idx], y[val_idx]),\n",
        "        sample_weight=sw_train,\n",
        "    )\n",
        "\n",
        "    oof_cat[val_idx] = model.predict_proba(X_flat[val_idx])\n",
        "    holdout_cat += model.predict_proba(X_holdout_flat) / N_FOLDS\n",
        "\n",
        "    f1 = f1_score(y[val_idx], oof_cat[val_idx].argmax(1), average='macro')\n",
        "    print(f\"  Fold {fold_i} macro F1: {f1:.4f}\")\n",
        "\n",
        "oof_f1 = f1_score(y, oof_cat.argmax(1), average='macro')\n",
        "print(f\"\\nCatBoost OOF macro F1: {oof_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Level 1: Meta-Learner (Stacking)\n",
        "\n",
        "Stack the OOF probability predictions from all 4 base models (4 classes x 4 models = 16 features).\n",
        "Train a logistic regression meta-learner, and compare against simple weighted averaging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stack OOF predictions\n",
        "oof_stack = np.hstack([oof_lgbm, oof_xgb, oof_cnn, oof_cat])\n",
        "holdout_stack = np.hstack([holdout_lgbm, holdout_xgb, holdout_cnn, holdout_cat])\n",
        "\n",
        "print(f\"Stacked OOF shape: {oof_stack.shape}\")\n",
        "print(f\"Stacked holdout shape: {holdout_stack.shape}\")\n",
        "\n",
        "# Individual model OOF scores\n",
        "model_names = ['LightGBM', 'XGBoost', '1D-CNN', 'CatBoost']\n",
        "oof_preds_list = [oof_lgbm, oof_xgb, oof_cnn, oof_cat]\n",
        "print(\"\\nIndividual OOF scores:\")\n",
        "for name, oof in zip(model_names, oof_preds_list):\n",
        "    f1 = f1_score(y, oof.argmax(1), average='macro')\n",
        "    print(f\"  {name:12s}: macro F1 = {f1:.4f}\")\n",
        "\n",
        "# Meta-learner: Logistic Regression\n",
        "meta_model = LogisticRegression(\n",
        "    C=1.0,\n",
        "    max_iter=1000,\n",
        "    multi_class='multinomial',\n",
        "    solver='lbfgs',\n",
        "    class_weight='balanced',\n",
        "    random_state=SEED,\n",
        ")\n",
        "meta_model.fit(oof_stack, y)\n",
        "\n",
        "meta_oof_preds = meta_model.predict(oof_stack)\n",
        "meta_f1 = f1_score(y, meta_oof_preds, average='macro')\n",
        "print(f\"\\nStacked ensemble OOF macro F1: {meta_f1:.4f}\")\n",
        "print(f\"\\nClassification Report (OOF):\")\n",
        "print(classification_report(y, meta_oof_preds, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Weighted average (weights proportional to OOF F1)\n",
        "f1_weights = []\n",
        "for oof in oof_preds_list:\n",
        "    f1_weights.append(f1_score(y, oof.argmax(1), average='macro'))\n",
        "f1_weights = np.array(f1_weights)\n",
        "f1_weights = f1_weights / f1_weights.sum()\n",
        "\n",
        "print(\"Model weights (proportional to OOF F1):\")\n",
        "for name, w in zip(model_names, f1_weights):\n",
        "    print(f\"  {name:12s}: {w:.4f}\")\n",
        "\n",
        "avg_probs = sum(w * oof for w, oof in zip(f1_weights, oof_preds_list))\n",
        "avg_preds = avg_probs.argmax(1)\n",
        "avg_f1 = f1_score(y, avg_preds, average='macro')\n",
        "print(f\"\\nWeighted average OOF macro F1: {avg_f1:.4f}\")\n",
        "\n",
        "# Pick best approach\n",
        "if meta_f1 >= avg_f1:\n",
        "    print(f\"\\n-> Using meta-learner (F1={meta_f1:.4f} >= {avg_f1:.4f})\")\n",
        "    use_meta = True\n",
        "else:\n",
        "    print(f\"\\n-> Using weighted average (F1={avg_f1:.4f} > {meta_f1:.4f})\")\n",
        "    use_meta = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Holdout Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if use_meta:\n",
        "    holdout_preds = meta_model.predict(holdout_stack)\n",
        "    holdout_probs_final = meta_model.predict_proba(holdout_stack)\n",
        "else:\n",
        "    holdout_probs_final = sum(w * hp for w, hp in zip(f1_weights, [holdout_lgbm, holdout_xgb, holdout_cnn, holdout_cat]))\n",
        "    holdout_preds = holdout_probs_final.argmax(1)\n",
        "\n",
        "print(f\"Holdout prediction distribution:\")\n",
        "unique, counts = np.unique(holdout_preds, return_counts=True)\n",
        "for u, c in zip(unique, counts):\n",
        "    print(f\"  Label {u}: {c:>8} ({c/len(holdout_preds)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission = pd.DataFrame({\n",
        "    'ID': range(1, len(holdout_preds) + 1),\n",
        "    'predicted_label': holdout_preds.astype(int),\n",
        "})\n",
        "\n",
        "out_path = DATA_DIR / 'ensemble_stack_submission.csv'\n",
        "submission.to_csv(out_path, index=False)\n",
        "print(f\"Saved to {out_path}\")\n",
        "print(submission.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save all probabilities for potential further ensembling\n",
        "for name, h_probs in zip(model_names, [holdout_lgbm, holdout_xgb, holdout_cnn, holdout_cat]):\n",
        "    fname = f'{name.lower().replace(\" \", \"_\")}_holdout_probs_stack.csv'\n",
        "    pd.DataFrame(h_probs, columns=[f'prob_{i}' for i in range(4)]).to_csv(DATA_DIR / fname, index=False)\n",
        "print(\"All base model holdout probabilities saved.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
