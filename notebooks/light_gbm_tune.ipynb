{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LightGBM Hyperparameter Tuning with Optuna\n",
        "\n",
        "**MedHack Frontiers** — Optuna tunes LightGBM on precomputed features.\n",
        "\n",
        "## Setup (Google Colab)\n",
        "1. Upload these files to a folder in Google Drive (e.g. `MyDrive/medhack-frontiers/`):\n",
        "   - `train_features.parquet`, `test_features.parquet`, `holdout_features.parquet`\n",
        "   - `train_data.csv`, `test_data.csv` (required for FAR: encounter-level false alarm rate)\n",
        "   - `sample_submission.csv` (optional, for validation)\n",
        "2. Update `DRIVE_DATA_DIR` below to match your folder path.\n",
        "3. Run all cells.\n",
        "\n",
        "**Note:** Train and test are loaded separately (no concat) to avoid RAM issues. Multi-objective tuning: maximize Mean Macro-AUPRC, minimize False Alarm Rate (FAR = % of healthy encounters with ≥1 alarm at prob > 0.5)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies\n",
        "!pip install -q lightgbm scikit-learn optuna pandas pyarrow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Mount Google Drive and set data path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_DATA_DIR = '/content/drive/MyDrive/data'  # Update if your folder is elsewhere"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "from sklearn.metrics import f1_score, classification_report, average_precision_score, confusion_matrix\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DATA_DIR = Path(DRIVE_DATA_DIR)\n",
        "\n",
        "# Use all features (exclude label)\n",
        "EXCLUDE_COLS = [\"label\"]\n",
        "\n",
        "\n",
        "def macro_fpr(y_true, y_pred, n_classes=4):\n",
        "    \"\"\"Macro average false positive rate (one-vs-rest per class).\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=range(n_classes))\n",
        "    fprs = []\n",
        "    for i in range(n_classes):\n",
        "        fp = cm[:, i].sum() - cm[i, i]\n",
        "        n_neg = cm.sum() - cm[i, :].sum()\n",
        "        fprs.append(fp / n_neg if n_neg > 0 else 0)\n",
        "    return np.mean(fprs)\n",
        "\n",
        "\n",
        "def false_alarm_rate(y_true, y_prob, encounter_ids, threshold=0.5):\n",
        "    \"\"\"\n",
        "    FAR = % of all-negative (healthy) encounters where predicted prob for\n",
        "    deterioration (classes 1,2,3) crosses threshold at least once.\n",
        "    \"\"\"\n",
        "    # Prob of any positive/deterioration class (1, 2, 3)\n",
        "    prob_positive = y_prob[:, 1:].sum(axis=1)\n",
        "    df = pd.DataFrame({\"encounter_id\": encounter_ids, \"label\": y_true, \"prob_pos\": prob_positive})\n",
        "    # Healthy encounters: all labels are 0\n",
        "    healthy = df.groupby(\"encounter_id\").agg({\"label\": \"max\", \"prob_pos\": \"max\"}).reset_index()\n",
        "    healthy_encounters = healthy[healthy[\"label\"] == 0]\n",
        "    n_healthy = len(healthy_encounters)\n",
        "    if n_healthy == 0:\n",
        "        return 0.0\n",
        "    n_false_alarms = (healthy_encounters[\"prob_pos\"] >= threshold).sum()\n",
        "    return n_false_alarms / n_healthy\n",
        "\n",
        "\n",
        "print(\"Imports OK\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load parquet features from Drive (train and test separately to avoid RAM issues)\n",
        "print(\"Loading features from Google Drive...\")\n",
        "train = pd.read_parquet(DATA_DIR / \"train_features.parquet\")\n",
        "test = pd.read_parquet(DATA_DIR / \"test_features.parquet\")\n",
        "holdout = pd.read_parquet(DATA_DIR / \"holdout_features.parquet\")\n",
        "\n",
        "feature_cols = [c for c in train.columns if c not in EXCLUDE_COLS]\n",
        "X_train = train[feature_cols].copy()\n",
        "y_train = train[\"label\"].astype(int)\n",
        "X_test = test[feature_cols].copy()\n",
        "y_test = test[\"label\"].astype(int)\n",
        "\n",
        "print(f\"Train: {X_train.shape}, Test: {X_test.shape}, Holdout: {holdout.shape}\")\n",
        "print(f\"Features used: {len(feature_cols)}\")\n",
        "print(f\"\\nTrain label distribution:\\n{y_train.value_counts().sort_index()}\")\n",
        "\n",
        "# Class weights from train (same as medhack_pipeline)\n",
        "class_counts = y_train.value_counts().sort_index()\n",
        "total = len(y_train)\n",
        "class_weights = {c: total / (len(class_counts) * count) for c, count in class_counts.items()}\n",
        "class_weights[2] = class_weights[2] * 3.0\n",
        "class_weights[3] = class_weights[3] * 2.0\n",
        "sample_weights = y_train.map(class_weights).values\n",
        "print(f\"\\nClass weights: {class_weights}\")\n",
        "\n",
        "# Load encounter_ids for FAR (test set only)\n",
        "print(\"Loading encounter_ids for FAR...\")\n",
        "test_encounter_ids = pd.read_csv(DATA_DIR / \"test_data.csv\", usecols=[\"encounter_id\"])[\"encounter_id\"]\n",
        "assert len(test_encounter_ids) == len(X_test), f\"Row mismatch: {len(test_encounter_ids)} vs {len(X_test)}\"\n",
        "\n",
        "# Optuna threshold for FAR\n",
        "FAR_THRESHOLD = 0.5\n",
        "\n",
        "del train, test\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optuna Multi-Objective Hyperparameter Tuning\n",
        "\n",
        "**Objective A (Maximize):** Mean Macro-AUPRC  \n",
        "**Objective B (Minimize):** False Alarm Rate — % of healthy (all-negative) encounters where prob(deterioration) ≥ 0.5 at least once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def objective(trial):\n",
        "    params = {\n",
        "        \"objective\": \"multiclass\",\n",
        "        \"num_class\": 4,\n",
        "        \"metric\": \"multi_logloss\",\n",
        "        \"boosting_type\": \"gbdt\",\n",
        "        \"verbosity\": -1,\n",
        "        \"n_jobs\": -1,\n",
        "        \"seed\": 42,\n",
        "        # Tunable hyperparameters (tuned for ~2.1M rows, 259 features)\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 63, 511),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
        "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.3, 0.8),\n",
        "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 0.9),\n",
        "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 3, 10),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 50, 300),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-2, 10.0, log=True),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-2, 10.0, log=True),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 6, 12),\n",
        "    }\n",
        "\n",
        "    dtrain = lgb.Dataset(X_train, label=y_train, weight=sample_weights)\n",
        "    dval = lgb.Dataset(X_test, label=y_test, reference=dtrain)\n",
        "\n",
        "    model = lgb.train(\n",
        "        params, dtrain,\n",
        "        num_boost_round=2000,\n",
        "        valid_sets=[dval],\n",
        "        callbacks=[lgb.early_stopping(50, verbose=False), lgb.log_evaluation(0)],\n",
        "    )\n",
        "\n",
        "    y_prob = model.predict(X_test)\n",
        "    y_bin = label_binarize(y_test, classes=[0, 1, 2, 3])\n",
        "    mean_auprc = average_precision_score(y_bin, y_prob, average=\"macro\")\n",
        "    far = false_alarm_rate(y_test, y_prob, test_encounter_ids, threshold=FAR_THRESHOLD)\n",
        "\n",
        "    trial.set_user_attr(\"mean_auprc\", mean_auprc)\n",
        "    trial.set_user_attr(\"far\", far)\n",
        "    trial.set_user_attr(\"best_iteration\", model.best_iteration)\n",
        "    # Multi-objective: (maximize AUPRC, minimize FAR)\n",
        "    return mean_auprc, far"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "N_TRIALS = 50  # Increase for more thorough search\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "study = optuna.create_study(\n",
        "    directions=[\"maximize\", \"minimize\"],  # max AUPRC, min FAR\n",
        "    study_name=\"lgbm_medhack_multi\",\n",
        ")\n",
        "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
        "\n",
        "print(f\"\\n--- Pareto Front ({len(study.best_trials)} trials) ---\")\n",
        "for i, t in enumerate(study.best_trials):\n",
        "    print(f\"  Trial {t.number}: AUPRC={t.values[0]:.4f}, FAR={t.values[1]:.4f}\")\n",
        "# Select trial with highest AUPRC from Pareto front (or pick by preference)\n",
        "best_trial = max(study.best_trials, key=lambda t: t.values[0])\n",
        "print(f\"\\nSelected (highest AUPRC): Trial {best_trial.number}\")\n",
        "print(f\"  Mean AUPRC: {best_trial.user_attrs['mean_auprc']:.4f}\")\n",
        "print(f\"  FAR: {best_trial.user_attrs['far']:.4f}\")\n",
        "print(f\"  Best iteration: {best_trial.user_attrs['best_iteration']}\")\n",
        "print(f\"\\nBest params:\")\n",
        "for k, v in best_trial.params.items():\n",
        "    print(f\"  {k}: {v}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional: visualization (requires plotly)\n",
        "try:\n",
        "    from optuna.visualization import plot_pareto_front, plot_param_importances\n",
        "    fig1 = plot_pareto_front(study, target_names=[\"AUPRC\", \"FAR\"])\n",
        "    fig1.show()\n",
        "    fig2 = plot_param_importances(study, target=lambda t: t.values[0])  # by AUPRC\n",
        "    fig2.show()\n",
        "except Exception as e:\n",
        "    print(f\"Visualization skipped: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Final Model and Save\n",
        "\n",
        "Trains on train set with best params, then saves model and feature columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build final params from selected Pareto-optimal trial\n",
        "best_trial = max(study.best_trials, key=lambda t: t.values[0])\n",
        "best_params = best_trial.params.copy()\n",
        "best_params.update({\n",
        "    \"objective\": \"multiclass\",\n",
        "    \"num_class\": 4,\n",
        "    \"metric\": \"multi_logloss\",\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"verbosity\": -1,\n",
        "    \"n_jobs\": -1,\n",
        "    \"seed\": 42,\n",
        "})\n",
        "\n",
        "best_iteration = best_trial.user_attrs[\"best_iteration\"]\n",
        "best_iteration = max(best_iteration, 500)  # Ensure minimum rounds\n",
        "\n",
        "print(f\"Training final model on train ({best_iteration} rounds)...\")\n",
        "dtrain_full = lgb.Dataset(X_train, label=y_train, weight=sample_weights)\n",
        "final_model = lgb.train(best_params, dtrain_full, num_boost_round=best_iteration)\n",
        "\n",
        "# Save to Drive\n",
        "out_dir = Path(DRIVE_DATA_DIR)\n",
        "with open(out_dir / \"lgb_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(final_model, f)\n",
        "with open(out_dir / \"feature_cols.pkl\", \"wb\") as f:\n",
        "    pickle.dump(feature_cols, f)\n",
        "\n",
        "print(f\"Saved to {out_dir}: lgb_model.pkl, feature_cols.pkl\")\n",
        "\n",
        "# Evaluate on test\n",
        "test_preds = final_model.predict(X_test).argmax(axis=1)\n",
        "test_f1 = f1_score(y_test, test_preds, average=\"macro\")\n",
        "test_auprc = average_precision_score(label_binarize(y_test, classes=[0,1,2,3]), final_model.predict(X_test), average=\"macro\")\n",
        "test_far = false_alarm_rate(y_test, final_model.predict(X_test), test_encounter_ids, threshold=FAR_THRESHOLD)\n",
        "print(f\"\\nTest: Macro F1={test_f1:.4f}, AUPRC={test_auprc:.4f}, FAR={test_far:.4f}\")\n",
        "print(classification_report(y_test, test_preds, target_names=[\"Normal\", \"Warning\", \"Crisis\", \"Death\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Submission\n",
        "\n",
        "Predict on holdout and save `submission.csv` to Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Loading model and feature columns...\")\n",
        "with open(DATA_DIR / \"lgb_model.pkl\", \"rb\") as f:\n",
        "    model = pickle.load(f)\n",
        "with open(DATA_DIR / \"feature_cols.pkl\", \"rb\") as f:\n",
        "    feature_cols = pickle.load(f)\n",
        "\n",
        "for col in feature_cols:\n",
        "    if col not in holdout.columns:\n",
        "        holdout[col] = 0\n",
        "\n",
        "X_holdout = holdout[feature_cols]\n",
        "print(f\"Holdout shape: {X_holdout.shape}\")\n",
        "\n",
        "proba = model.predict(X_holdout)\n",
        "predictions = proba.argmax(axis=1)\n",
        "\n",
        "print(f\"\\nPrediction distribution:\")\n",
        "for label in sorted(np.unique(predictions)):\n",
        "    count = (predictions == label).sum()\n",
        "    print(f\"  Label {label}: {count:,} ({count/len(predictions)*100:.1f}%)\")\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"ID\": np.arange(1, len(predictions) + 1),\n",
        "    \"predicted_label\": predictions,\n",
        "})\n",
        "sub_path = Path(DRIVE_DATA_DIR) / \"submission.csv\"\n",
        "submission.to_csv(sub_path, index=False)\n",
        "print(f\"\\nSaved {sub_path}\")\n",
        "\n",
        "# Verify if sample_submission exists\n",
        "sample_path = DATA_DIR / \"sample_submission.csv\"\n",
        "if sample_path.exists():\n",
        "    sample = pd.read_csv(sample_path)\n",
        "    assert len(submission) == len(sample), f\"Row mismatch: {len(submission)} vs {len(sample)}\"\n",
        "    assert list(submission.columns) == list(sample.columns), \"Column mismatch\"\n",
        "    print(\"Format verified against sample_submission.csv!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Download submission to local machine (optional)\n",
        "from google.colab import files\n",
        "files.download(str(Path(DRIVE_DATA_DIR) / \"submission.csv\"))\n",
        "print(\"Downloaded submission.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}