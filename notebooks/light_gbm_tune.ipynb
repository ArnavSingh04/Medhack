{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LightGBM Hyperparameter Tuning with Optuna\n",
        "\n",
        "**MedHack Frontiers** â€” Optuna tunes LightGBM on precomputed features.\n",
        "\n",
        "## Setup (Google Colab)\n",
        "1. Upload these files to a folder in Google Drive (e.g. `MyDrive/medhack-frontiers/`):\n",
        "   - `train_features.parquet`, `test_features.parquet`, `holdout_features.parquet`\n",
        "   - `train_data.csv`, `test_data.csv` (for encounter_ids used in GroupKFold)\n",
        "   - `sample_submission.csv` (optional, for validation)\n",
        "2. Update `DRIVE_DATA_DIR` below to match your folder path.\n",
        "3. Run all cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q lightgbm scikit-learn optuna pandas pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive and set data path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_DATA_DIR = '/content/drive/MyDrive/data'  # Update if your folder is elsewhere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import optuna\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DATA_DIR = Path(DRIVE_DATA_DIR)\n",
        "\n",
        "# Use all features (exclude label)\n",
        "EXCLUDE_COLS = [\"label\"]\n",
        "\n",
        "print(\"Imports OK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load parquet features from Drive\n",
        "print(\"Loading features from Google Drive...\")\n",
        "train = pd.read_parquet(DATA_DIR / \"train_features.parquet\")\n",
        "test = pd.read_parquet(DATA_DIR / \"test_features.parquet\")\n",
        "holdout = pd.read_parquet(DATA_DIR / \"holdout_features.parquet\")\n",
        "\n",
        "combined = pd.concat([train, test], ignore_index=True)\n",
        "del train, test\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Combined (train+test): {combined.shape}\")\n",
        "print(f\"Holdout: {holdout.shape}\")\n",
        "\n",
        "y = combined[\"label\"].astype(int)\n",
        "feature_cols = [c for c in combined.columns if c not in EXCLUDE_COLS]\n",
        "X = combined[feature_cols].copy()\n",
        "\n",
        "print(f\"Features used: {len(feature_cols)}\")\n",
        "print(f\"\\nLabel distribution:\\n{y.value_counts().sort_index()}\")\n",
        "\n",
        "# Class weights (same as medhack_pipeline)\n",
        "class_counts = y.value_counts().sort_index()\n",
        "total = len(y)\n",
        "class_weights = {c: total / (len(class_counts) * count) for c, count in class_counts.items()}\n",
        "class_weights[2] = class_weights[2] * 3.0\n",
        "class_weights[3] = class_weights[3] * 2.0\n",
        "sample_weights = y.map(class_weights).values\n",
        "print(f\"\\nClass weights: {class_weights}\")\n",
        "\n",
        "# Load encounter_ids for GroupKFold (prevents leakage across encounters)\n",
        "print(\"Loading encounter_ids for GroupKFold...\")\n",
        "train_raw = pd.read_csv(DATA_DIR / \"train_data.csv\", usecols=[\"encounter_id\"])\n",
        "test_raw = pd.read_csv(DATA_DIR / \"test_data.csv\", usecols=[\"encounter_id\"])\n",
        "encounter_ids = pd.concat([train_raw, test_raw], ignore_index=True)[\"encounter_id\"]\n",
        "del train_raw, test_raw\n",
        "gc.collect()\n",
        "\n",
        "assert len(encounter_ids) == len(X), f\"Row mismatch: encounter_ids {len(encounter_ids)} vs X {len(X)}\"\n",
        "print(f\"Encounter IDs loaded: {len(encounter_ids)}\")\n",
        "\n",
        "del combined\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optuna Hyperparameter Tuning\n",
        "\n",
        "Tunes main LightGBM hyperparameters using 5-fold GroupKFold CV. Macro F1 is maximized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    params = {\n",
        "        \"objective\": \"multiclass\",\n",
        "        \"num_class\": 4,\n",
        "        \"metric\": \"multi_logloss\",\n",
        "        \"boosting_type\": \"gbdt\",\n",
        "        \"verbosity\": -1,\n",
        "        \"n_jobs\": -1,\n",
        "        \"seed\": 42,\n",
        "        # Tunable hyperparameters (tuned for ~2.1M rows, 259 features)\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 63, 511),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
        "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.3, 0.8),\n",
        "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 0.9),\n",
        "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 3, 10),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 50, 300),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-2, 10.0, log=True),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-2, 10.0, log=True),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 6, 12),\n",
        "    }\n",
        "\n",
        "    gkf = GroupKFold(n_splits=5)\n",
        "    fold_scores = []\n",
        "    best_iterations = []\n",
        "\n",
        "    for train_idx, val_idx in gkf.split(X, y, groups=encounter_ids):\n",
        "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "        w_tr = sample_weights[train_idx]\n",
        "\n",
        "        dtrain = lgb.Dataset(X_tr, label=y_tr, weight=w_tr)\n",
        "        dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n",
        "\n",
        "        model = lgb.train(\n",
        "            params, dtrain,\n",
        "            num_boost_round=2000,\n",
        "            valid_sets=[dval],\n",
        "            callbacks=[lgb.early_stopping(50, verbose=False), lgb.log_evaluation(0)],\n",
        "        )\n",
        "\n",
        "        preds = model.predict(X_val).argmax(axis=1)\n",
        "        macro_f1 = f1_score(y_val, preds, average=\"macro\")\n",
        "        fold_scores.append(macro_f1)\n",
        "        best_iterations.append(model.best_iteration)\n",
        "\n",
        "    mean_f1 = np.mean(fold_scores)\n",
        "    trial.set_user_attr(\"fold_scores\", fold_scores)\n",
        "    trial.set_user_attr(\"best_iterations\", best_iterations)\n",
        "    trial.set_user_attr(\"mean_best_iteration\", int(np.mean(best_iterations)))\n",
        "    return mean_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "N_TRIALS = 50  # Increase for more thorough search\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "study = optuna.create_study(direction=\"maximize\", study_name=\"lgbm_medhack\")\n",
        "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
        "\n",
        "print(f\"\\n--- Best Trial ---\")\n",
        "print(f\"Mean Macro F1: {study.best_trial.value:.4f}\")\n",
        "print(f\"Fold scores: {[f'{s:.4f}' for s in study.best_trial.user_attrs['fold_scores']]}\")\n",
        "print(f\"Mean best iteration: {study.best_trial.user_attrs['mean_best_iteration']}\")\n",
        "print(f\"\\nBest params:\")\n",
        "for k, v in study.best_trial.params.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: visualization (requires plotly)\n",
        "try:\n",
        "    from optuna.visualization import plot_optimization_history, plot_param_importances\n",
        "    fig1 = plot_optimization_history(study)\n",
        "    fig1.show()\n",
        "    fig2 = plot_param_importances(study)\n",
        "    fig2.show()\n",
        "except Exception as e:\n",
        "    print(f\"Visualization skipped: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Final Model and Save\n",
        "\n",
        "Refits on all train+test data with best params, then saves model and feature columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build final params from best trial\n",
        "best_params = study.best_trial.params.copy()\n",
        "best_params.update({\n",
        "    \"objective\": \"multiclass\",\n",
        "    \"num_class\": 4,\n",
        "    \"metric\": \"multi_logloss\",\n",
        "    \"boosting_type\": \"gbdt\",\n",
        "    \"verbosity\": -1,\n",
        "    \"n_jobs\": -1,\n",
        "    \"seed\": 42,\n",
        "})\n",
        "\n",
        "best_iteration = study.best_trial.user_attrs[\"mean_best_iteration\"]\n",
        "best_iteration = max(best_iteration, 500)  # Ensure minimum rounds\n",
        "\n",
        "print(f\"Training final model ({best_iteration} rounds)...\")\n",
        "dtrain_full = lgb.Dataset(X, label=y, weight=sample_weights)\n",
        "final_model = lgb.train(best_params, dtrain_full, num_boost_round=best_iteration)\n",
        "\n",
        "# Save to Drive\n",
        "out_dir = Path(DRIVE_DATA_DIR)\n",
        "with open(out_dir / \"lgb_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump(final_model, f)\n",
        "with open(out_dir / \"feature_cols.pkl\", \"wb\") as f:\n",
        "    pickle.dump(feature_cols, f)\n",
        "\n",
        "print(f\"Saved to {out_dir}: lgb_model.pkl, feature_cols.pkl\")\n",
        "\n",
        "# Sanity check\n",
        "train_preds = final_model.predict(X).argmax(axis=1)\n",
        "train_f1 = f1_score(y, train_preds, average=\"macro\")\n",
        "print(f\"\\nTrain Macro F1 (sanity): {train_f1:.4f}\")\n",
        "print(classification_report(y, train_preds, target_names=[\"Normal\", \"Warning\", \"Crisis\", \"Death\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Submission\n",
        "\n",
        "Predict on holdout and save `submission.csv` to Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading model and feature columns...\")\n",
        "with open(DATA_DIR / \"lgb_model.pkl\", \"rb\") as f:\n",
        "    model = pickle.load(f)\n",
        "with open(DATA_DIR / \"feature_cols.pkl\", \"rb\") as f:\n",
        "    feature_cols = pickle.load(f)\n",
        "\n",
        "for col in feature_cols:\n",
        "    if col not in holdout.columns:\n",
        "        holdout[col] = 0\n",
        "\n",
        "X_holdout = holdout[feature_cols]\n",
        "print(f\"Holdout shape: {X_holdout.shape}\")\n",
        "\n",
        "proba = model.predict(X_holdout)\n",
        "predictions = proba.argmax(axis=1)\n",
        "\n",
        "print(f\"\\nPrediction distribution:\")\n",
        "for label in sorted(np.unique(predictions)):\n",
        "    count = (predictions == label).sum()\n",
        "    print(f\"  Label {label}: {count:,} ({count/len(predictions)*100:.1f}%)\")\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    \"ID\": np.arange(1, len(predictions) + 1),\n",
        "    \"predicted_label\": predictions,\n",
        "})\n",
        "sub_path = Path(DRIVE_DATA_DIR) / \"submission.csv\"\n",
        "submission.to_csv(sub_path, index=False)\n",
        "print(f\"\\nSaved {sub_path}\")\n",
        "\n",
        "# Verify if sample_submission exists\n",
        "sample_path = DATA_DIR / \"sample_submission.csv\"\n",
        "if sample_path.exists():\n",
        "    sample = pd.read_csv(sample_path)\n",
        "    assert len(submission) == len(sample), f\"Row mismatch: {len(submission)} vs {len(sample)}\"\n",
        "    assert list(submission.columns) == list(sample.columns), \"Column mismatch\"\n",
        "    print(\"Format verified against sample_submission.csv!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download submission to local machine (optional)\n",
        "from google.colab import files\n",
        "files.download(str(Path(DRIVE_DATA_DIR) / \"submission.csv\"))\n",
        "print(\"Downloaded submission.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
