{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + Patient Features Fusion — Google Colab\n",
    "\n",
    "Implements the dual-branch architecture from `specs/lstm_patient_fusion_spec.md`.\n",
    "\n",
    "## Key design decisions for imbalanced classes\n",
    "\n",
    "| Metric | Why |\n",
    "|---|---|\n",
    "| **Macro ROC AUC** | Competition metric; treats all 4 classes equally regardless of frequency |\n",
    "| **Macro F1** | Training monitor; captures the precision/recall tradeoff — better than accuracy |\n",
    "| **Per-class F1** | Reveals which deterioration labels (esp. rare 2 & 3) the model handles worst |\n",
    "| **AUPRC** | Precision-recall curve area; more sensitive than ROC AUC when positives are rare |\n",
    "\n",
    "> **Why not accuracy?** With imbalanced classes a model that always predicts the majority class scores high accuracy but is clinically useless. F1 penalises both missing real events (low recall) and over-alerting (low precision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch gdown pandas scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data from Google Drive\n",
    "\n",
    "Loads data from the [shared Drive folder](https://drive.google.com/drive/folders/13NvOvSW1W0shkAxnYyZJlHFqokhHjUeI).\n",
    "- **Colab**: Mounts Drive. Update `COLAB_DATA_PATH` to match your folder after mounting.\n",
    "- **Local**: Downloads via gdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import gdown\n",
    "\n",
    "GOOGLE_DRIVE_FOLDER_ID = \"13NvOvSW1W0shkAxnYyZJlHFqokhHjUeI\"\n",
    "GOOGLE_DRIVE_URL = f\"https://drive.google.com/drive/folders/{GOOGLE_DRIVE_FOLDER_ID}\"\n",
    "COLAB_DATA_PATH = \"/content/drive/MyDrive/Medhack_data\"  # <-- update if needed\n",
    "\n",
    "def get_data_dir() -> Path:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount(\"/content/drive\", force_remount=False)\n",
    "        data_dir = Path(COLAB_DATA_PATH)\n",
    "        if not (data_dir / \"train_data_lagged.csv\").exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"train_data_lagged.csv not found in {data_dir}. \"\n",
    "                \"Update COLAB_DATA_PATH to match your Drive folder.\"\n",
    "            )\n",
    "        return data_dir\n",
    "    except ImportError:\n",
    "        out = Path(\"data_from_drive\")\n",
    "        out.mkdir(exist_ok=True)\n",
    "        gdown.download_folder(url=GOOGLE_DRIVE_URL, output=str(out), quiet=False)\n",
    "        if (out / \"train_data_lagged.csv\").exists():\n",
    "            return out\n",
    "        for sub in out.iterdir():\n",
    "            if sub.is_dir() and (sub / \"train_data_lagged.csv\").exists():\n",
    "                return sub\n",
    "        raise FileNotFoundError(f\"train_data_lagged.csv not found in {out}.\")\n",
    "\n",
    "DATA_DIR = get_data_dir()\n",
    "print(f\"DATA_DIR = {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, label_binarize\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, classification_report,\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    precision_recall_curve, average_precision_score,\n",
    ")\n",
    "\n",
    "# Device: prefer CUDA (Colab GPU) > MPS (Apple Silicon) > CPU\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "MODELS_DIR = Path('/content/models') if Path('/content').exists() else Path('../models')\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "VITAL_COLS = ['heart_rate', 'systolic_bp', 'diastolic_bp', 'respiratory_rate', 'oxygen_saturation']\n",
    "N_LAGS = 6\n",
    "TRAIN_SAMPLE_SIZE = 500_000\n",
    "RANDOM_STATE = 42\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "NUMERIC_PATIENT_COLS     = ['age', 'bmi', 'pain_score']\n",
    "CATEGORICAL_PATIENT_COLS = ['race', 'ethnicity']\n",
    "GENDER_COL = 'gender'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Reshape Vital Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_lagged_to_sequences(df: pd.DataFrame, vital_cols: list, n_lags: int) -> np.ndarray:\n",
    "    \"\"\"Reshape pre-computed lag columns into (n_samples, timesteps, n_vitals).\n",
    "    Timestep order: lag_n, ..., lag_1, current (oldest → newest).\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for col in vital_cols:\n",
    "        lag_cols = [f'{col}_lag{i}' for i in range(n_lags, 0, -1)]\n",
    "        sequences.append(df[lag_cols + [col]].values)\n",
    "    return np.stack(sequences, axis=-1)  # (n_samples, 7, 5)\n",
    "\n",
    "\n",
    "train_raw = pd.read_csv(DATA_DIR / 'train_data_lagged.csv')\n",
    "test_raw  = pd.read_csv(DATA_DIR / 'test_data_lagged.csv')\n",
    "\n",
    "X_ts_train_full = reshape_lagged_to_sequences(train_raw, VITAL_COLS, N_LAGS)\n",
    "y_train_full    = train_raw['label'].values\n",
    "X_ts_test       = reshape_lagged_to_sequences(test_raw,  VITAL_COLS, N_LAGS)\n",
    "y_test          = test_raw['label'].values\n",
    "\n",
    "print(f\"Train: {X_ts_train_full.shape}  |  Test: {X_ts_test.shape}\")\n",
    "\n",
    "# Show class distribution — this tells us why accuracy is misleading\n",
    "print(\"\\nClass distribution (train):\")\n",
    "counts = pd.Series(y_train_full).value_counts().sort_index()\n",
    "for cls, cnt in counts.items():\n",
    "    print(f\"  Label {cls}: {cnt:>8,}  ({100*cnt/len(y_train_full):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Preprocess Patient Features (Tier 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = pd.read_csv(DATA_DIR / 'patients.csv')\n",
    "print(f\"patients.csv: {patients.shape}\")\n",
    "print(patients.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_gender(series: pd.Series) -> np.ndarray:\n",
    "    mapping = {'M': 0, 'Male': 0, 'male': 0, 'F': 1, 'Female': 1, 'female': 1}\n",
    "    return series.map(mapping).fillna(-1).astype(float).values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "class PatientFeaturePreprocessor:\n",
    "    \"\"\"Fit on train patients; transform any split.\"\"\"\n",
    "\n",
    "    def __init__(self, numeric_cols=NUMERIC_PATIENT_COLS,\n",
    "                 categorical_cols=CATEGORICAL_PATIENT_COLS, gender_col=GENDER_COL):\n",
    "        self.numeric_cols = numeric_cols\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.gender_col = gender_col\n",
    "        self.num_scaler = StandardScaler()\n",
    "        self.cat_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        self._num_medians: dict = {}\n",
    "        self._cat_cols_present: list = []\n",
    "\n",
    "    def fit(self, df: pd.DataFrame) -> 'PatientFeaturePreprocessor':\n",
    "        for col in self.numeric_cols:\n",
    "            self._num_medians[col] = df[col].median() if col in df.columns else 0.0\n",
    "        self.num_scaler.fit(self._impute_numeric(df))\n",
    "        self._cat_cols_present = [c for c in self.categorical_cols if c in df.columns]\n",
    "        if self._cat_cols_present:\n",
    "            self.cat_encoder.fit(df[self._cat_cols_present].fillna('unknown').astype(str))\n",
    "        return self\n",
    "\n",
    "    def _impute_numeric(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        parts = []\n",
    "        for col in self.numeric_cols:\n",
    "            vals = df[col].fillna(self._num_medians[col]).values if col in df.columns \\\n",
    "                   else np.full(len(df), self._num_medians.get(col, 0.0))\n",
    "            parts.append(vals.reshape(-1, 1))\n",
    "        return np.hstack(parts).astype(float)\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        parts = [self.num_scaler.transform(self._impute_numeric(df))]\n",
    "        parts.append(encode_gender(df[self.gender_col]) if self.gender_col in df.columns\n",
    "                     else np.full((len(df), 1), -1.0))\n",
    "        if self._cat_cols_present:\n",
    "            parts.append(self.cat_encoder.transform(\n",
    "                df[self._cat_cols_present].fillna('unknown').astype(str)\n",
    "            ))\n",
    "        return np.hstack(parts).astype(np.float32)\n",
    "\n",
    "\n",
    "train_patients = patients[patients['encounter_id'].isin(train_raw['encounter_id'])]\n",
    "pat_prep = PatientFeaturePreprocessor()\n",
    "pat_prep.fit(train_patients)\n",
    "n_static = pat_prep.transform(train_patients.head(1)).shape[1]\n",
    "print(f\"n_static features: {n_static}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Join, Sample, and Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_static_features(lagged_df, patients_df, preprocessor):\n",
    "    pat_cols = ['encounter_id'] + [\n",
    "        c for c in preprocessor.numeric_cols + preprocessor.categorical_cols + [preprocessor.gender_col]\n",
    "        if c in patients_df.columns\n",
    "    ]\n",
    "    merged = lagged_df[['encounter_id']].merge(patients_df[pat_cols], on='encounter_id', how='left')\n",
    "    return preprocessor.transform(merged)\n",
    "\n",
    "\n",
    "X_static_train_full = build_static_features(train_raw, patients, pat_prep)\n",
    "X_static_test       = build_static_features(test_raw,  patients, pat_prep)\n",
    "\n",
    "# Stratified sample\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, train_size=TRAIN_SAMPLE_SIZE, random_state=RANDOM_STATE)\n",
    "idx, _ = next(splitter.split(X_ts_train_full, y_train_full))\n",
    "X_ts_train   = X_ts_train_full[idx]\n",
    "X_stat_train = X_static_train_full[idx]\n",
    "y_train      = y_train_full[idx]\n",
    "\n",
    "# Scale vitals (fit on train only)\n",
    "n_samples, n_timesteps, n_vitals = X_ts_train.shape\n",
    "ts_scaler = StandardScaler()\n",
    "ts_scaler.fit(X_ts_train.reshape(-1, n_vitals))\n",
    "\n",
    "X_ts_train_sc = ts_scaler.transform(X_ts_train.reshape(-1, n_vitals)).reshape(n_samples, n_timesteps, n_vitals)\n",
    "X_ts_test_sc  = ts_scaler.transform(X_ts_test.reshape(-1, n_vitals)).reshape(X_ts_test.shape[0], n_timesteps, n_vitals)\n",
    "\n",
    "print(f\"Train: ts={X_ts_train_sc.shape}, static={X_stat_train.shape}\")\n",
    "print(f\"Test:  ts={X_ts_test_sc.shape},  static={X_static_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMFusionModel(nn.Module):\n",
    "    def __init__(self, n_vitals: int, n_static: int, n_classes: int = 4):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(n_vitals, 64, batch_first=True)\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "        self.lstm2 = nn.LSTM(64, 32, batch_first=True)\n",
    "        self.drop2 = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32 + n_static, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_ts: torch.Tensor, x_static: torch.Tensor) -> torch.Tensor:\n",
    "        out, _ = self.lstm1(x_ts)\n",
    "        out = self.drop1(out)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = out[:, -1, :]  # last timestep → (batch, 32)\n",
    "        out = self.drop2(out)\n",
    "        return self.classifier(torch.cat([out, x_static], dim=1))\n",
    "\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "model = LSTMFusionModel(n_vitals, n_static, n_classes).to(DEVICE)\n",
    "print(model)\n",
    "print(f\"\\nTrainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train\n",
    "\n",
    "### Why train loss can rise while val loss falls\n",
    "Class weights penalize minority-class errors heavily (up to ~10×). In early epochs the model\n",
    "shifts away from always predicting the majority class — this causes more minority-class mistakes\n",
    "which the weighted loss inflates. Val loss falls because the model is genuinely improving on\n",
    "rare classes. The pattern stabilises after a few epochs.\n",
    "\n",
    "### What to monitor\n",
    "Watch **val macro F1**, not val accuracy. F1 is the harmonic mean of precision and recall —\n",
    "it tells you whether the model is actually catching deterioration events vs. just over-alerting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse-frequency class weights\n",
    "class_counts = np.bincount(y_train)\n",
    "class_weights_np = len(y_train) / (n_classes * class_counts)\n",
    "print(\"Class weights:\", {i: round(w, 3) for i, w in enumerate(class_weights_np)})\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights_np, dtype=torch.float32).to(DEVICE))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2, min_lr=1e-5)\n",
    "\n",
    "def make_loader(X_ts, X_stat, y, batch_size=512, shuffle=True):\n",
    "    ds = TensorDataset(\n",
    "        torch.tensor(X_ts,   dtype=torch.float32),\n",
    "        torch.tensor(X_stat, dtype=torch.float32),\n",
    "        torch.tensor(y,      dtype=torch.long),\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, pin_memory=(DEVICE.type == 'cuda'))\n",
    "\n",
    "val_split = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=RANDOM_STATE)\n",
    "tr_idx, val_idx = next(val_split.split(X_ts_train_sc, y_train))\n",
    "\n",
    "train_loader = make_loader(X_ts_train_sc[tr_idx],  X_stat_train[tr_idx],  y_train[tr_idx])\n",
    "val_loader   = make_loader(X_ts_train_sc[val_idx], X_stat_train[val_idx], y_train[val_idx], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS  = 15\n",
    "PATIENCE = 3\n",
    "\n",
    "best_val_f1 = -1.0\n",
    "epochs_no_improve = 0\n",
    "best_state = None\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_f1_macro': []}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    # --- Train ---\n",
    "    model.train()\n",
    "    train_loss = train_total = 0\n",
    "    for X_ts_b, X_st_b, y_b in train_loader:\n",
    "        X_ts_b, X_st_b, y_b = X_ts_b.to(DEVICE), X_st_b.to(DEVICE), y_b.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(X_ts_b, X_st_b), y_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss  += loss.item() * len(y_b)\n",
    "        train_total += len(y_b)\n",
    "\n",
    "    # --- Validate: collect all predictions for F1 ---\n",
    "    model.eval()\n",
    "    val_loss = val_total = 0\n",
    "    val_preds, val_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_ts_b, X_st_b, y_b in val_loader:\n",
    "            X_ts_b, X_st_b, y_b = X_ts_b.to(DEVICE), X_st_b.to(DEVICE), y_b.to(DEVICE)\n",
    "            logits = model(X_ts_b, X_st_b)\n",
    "            val_loss  += criterion(logits, y_b).item() * len(y_b)\n",
    "            val_total += len(y_b)\n",
    "            val_preds.append(logits.argmax(1).cpu())\n",
    "            val_true.append(y_b.cpu())\n",
    "\n",
    "    t_loss   = train_loss / train_total\n",
    "    v_loss   = val_loss   / val_total\n",
    "    val_preds_np = torch.cat(val_preds).numpy()\n",
    "    val_true_np  = torch.cat(val_true).numpy()\n",
    "    v_f1 = f1_score(val_true_np, val_preds_np, average='macro', zero_division=0)\n",
    "\n",
    "    history['train_loss'].append(t_loss)\n",
    "    history['val_loss'].append(v_loss)\n",
    "    history['val_f1_macro'].append(v_f1)\n",
    "\n",
    "    scheduler.step(v_loss)\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch:02d}  train_loss={t_loss:.4f}  val_loss={v_loss:.4f}  \"\n",
    "          f\"val_f1_macro={v_f1:.4f}  lr={lr:.2e}\")\n",
    "\n",
    "    # Early stop on val macro F1 (maximise)\n",
    "    if v_f1 > best_val_f1:\n",
    "        best_val_f1 = v_f1\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch} (best val macro F1={best_val_f1:.4f})\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "torch.save(model.state_dict(), MODELS_DIR / 'lstm_patient_fusion.pt')\n",
    "print(f\"\\nBest model saved  |  val macro F1 = {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "axes[0].plot(history['train_loss'], label='train loss')\n",
    "axes[0].plot(history['val_loss'],   label='val loss')\n",
    "axes[0].set_title('Weighted Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history['val_f1_macro'], color='tab:green', label='val macro F1')\n",
    "axes[1].set_title('Val Macro F1 (↑ better)')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(model, X_ts_sc, X_stat, batch_size=512):\n",
    "    model.eval()\n",
    "    loader = make_loader(X_ts_sc, X_stat, np.zeros(len(X_ts_sc), dtype=np.int64),\n",
    "                         batch_size=batch_size, shuffle=False)\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for X_ts_b, X_st_b, _ in loader:\n",
    "            logits = model(X_ts_b.to(DEVICE), X_st_b.to(DEVICE))\n",
    "            probs.append(torch.softmax(logits, dim=1).cpu().numpy())\n",
    "    return np.vstack(probs)\n",
    "\n",
    "\n",
    "y_proba = predict_proba(model, X_ts_test_sc, X_static_test)\n",
    "y_pred  = y_proba.argmax(axis=1)\n",
    "\n",
    "# --- Primary metric ---\n",
    "roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr', average='macro')\n",
    "print(f\"Test ROC AUC (macro, OVR): {roc_auc:.4f}\")\n",
    "print()\n",
    "\n",
    "# --- Classification report: per-class precision, recall, F1 ---\n",
    "print(classification_report(y_test, y_pred,\n",
    "      target_names=[f'Label {i}' for i in range(n_classes)],\n",
    "      digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9a. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ConfusionMatrixDisplay(cm, display_labels=[f'Label {i}' for i in range(n_classes)]).plot(\n",
    "    ax=axes[0], colorbar=False)\n",
    "axes[0].set_title('Confusion Matrix (counts)')\n",
    "\n",
    "# Normalised by true label (row) — reveals per-class recall\n",
    "cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "ConfusionMatrixDisplay(cm_norm.round(2), display_labels=[f'Label {i}' for i in range(n_classes)]).plot(\n",
    "    ax=axes[1], colorbar=False)\n",
    "axes[1].set_title('Confusion Matrix (row-normalised = recall per class)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9b. Precision-Recall Curves (one vs. rest)\n",
    "\n",
    "For imbalanced classes AUPRC is more informative than ROC AUC:\n",
    "a random classifier has AUPRC ≈ class prevalence, so a rare class makes it easy to game ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_bin = label_binarize(y_test, classes=list(range(n_classes)))\n",
    "\n",
    "fig, axes = plt.subplots(1, n_classes, figsize=(5 * n_classes, 4))\n",
    "for cls in range(n_classes):\n",
    "    prec, rec, _ = precision_recall_curve(y_test_bin[:, cls], y_proba[:, cls])\n",
    "    auprc = average_precision_score(y_test_bin[:, cls], y_proba[:, cls])\n",
    "    prevalence = y_test_bin[:, cls].mean()\n",
    "    axes[cls].plot(rec, prec, lw=2)\n",
    "    axes[cls].axhline(prevalence, linestyle='--', color='grey', label=f'baseline (prevalence={prevalence:.2f})')\n",
    "    axes[cls].set_title(f'Label {cls} — AUPRC={auprc:.3f}')\n",
    "    axes[cls].set_xlabel('Recall')\n",
    "    axes[cls].set_ylabel('Precision')\n",
    "    axes[cls].set_ylim(0, 1)\n",
    "    axes[cls].legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Precision-Recall Curves (one-vs-rest)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPer-class summary:\")\n",
    "for cls in range(n_classes):\n",
    "    roc = roc_auc_score(y_test_bin[:, cls], y_proba[:, cls])\n",
    "    auprc = average_precision_score(y_test_bin[:, cls], y_proba[:, cls])\n",
    "    prev = y_test_bin[:, cls].mean()\n",
    "    print(f\"  Label {cls}:  ROC AUC={roc:.4f}  AUPRC={auprc:.4f}  prevalence={prev:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Holdout Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_raw = pd.read_csv(DATA_DIR / 'holdout_data_lagged.csv')\n",
    "\n",
    "X_ts_holdout = reshape_lagged_to_sequences(holdout_raw, VITAL_COLS, N_LAGS)\n",
    "X_ts_holdout_sc = ts_scaler.transform(\n",
    "    X_ts_holdout.reshape(-1, n_vitals)\n",
    ").reshape(X_ts_holdout.shape[0], n_timesteps, n_vitals)\n",
    "X_static_holdout = build_static_features(holdout_raw, patients, pat_prep)\n",
    "\n",
    "y_holdout_proba = predict_proba(model, X_ts_holdout_sc, X_static_holdout)\n",
    "print(f\"Holdout predictions shape: {y_holdout_proba.shape}\")\n",
    "\n",
    "if 'label' in holdout_raw.columns:\n",
    "    roc_auc_holdout = roc_auc_score(\n",
    "        holdout_raw['label'], y_holdout_proba, multi_class='ovr', average='macro'\n",
    "    )\n",
    "    print(f\"Holdout ROC AUC (macro, OVR): {roc_auc_holdout:.4f}\")\n",
    "else:\n",
    "    print(\"Holdout has no labels; predictions ready for submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(y_holdout_proba, columns=[f'label_{i}' for i in range(n_classes)])\n",
    "if 'encounter_id' in holdout_raw.columns:\n",
    "    submission.insert(0, 'encounter_id', holdout_raw['encounter_id'].values)\n",
    "\n",
    "out_path = DATA_DIR / 'lstm_fusion_holdout_predictions.csv'\n",
    "submission.to_csv(out_path, index=False)\n",
    "print(f\"Saved: {out_path}\")\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
