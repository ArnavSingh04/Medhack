{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM Baseline: Patient Label Prediction\n",
        "\n",
        "Train a unidirectional LSTM on lagged vital signs from `train_data_lagged.csv` and evaluate ROC AUC on `test_data_lagged.csv`.\n",
        "\n",
        "**Lags and LSTM**: With LSTM, you have two options for temporal input:\n",
        "1. **Raw time series** – Create sliding windows of consecutive rows (no explicit lag columns); the LSTM learns from the raw sequence.\n",
        "2. **Pre-computed lags** (this notebook) – The lagged CSV already encodes a lookback window. Each row has `lag6..lag1` and current values per vital. We reshape these into `(timesteps, channels)` so the LSTM receives a sequence of shape `(7, 5)` – 7 time points (oldest→newest) and 5 sensor channels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Install Dependencies (run once)\n",
        "\n",
        "Run the cell below to install required packages.\n",
        "\n",
        "**From terminal (alternative):**\n",
        "```bash\n",
        "pip install tensorflow gdown pandas scikit-learn\n",
        "```\n",
        "If using **Google Colab**, dependencies are preinstalled; you can skip this step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q tensorflow gdown pandas scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data from Google Drive\n",
        "\n",
        "Loads data from the [shared Drive folder](https://drive.google.com/drive/folders/13NvOvSW1W0shkAxnYyZJlHFqokhHjUeI).\n",
        "- **Google Colab**: Mounts Drive; set `COLAB_DATA_PATH` to your folder path after mounting.\n",
        "- **Local/Jupyter**: Downloads via gdown. Ensure the folder is shared so anyone with the link can view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import gdown\n",
        "\n",
        "GOOGLE_DRIVE_FOLDER_ID = \"13NvOvSW1W0shkAxnYyZJlHFqokhHjUeI\"\n",
        "GOOGLE_DRIVE_URL = f\"https://drive.google.com/drive/folders/{GOOGLE_DRIVE_FOLDER_ID}\"\n",
        "\n",
        "# For Colab: set this to the path of your data folder after mounting (e.g. /content/drive/MyDrive/Medhack_data)\n",
        "# For Shared Drives: /content/drive/Shareddrives/<YourDriveName>/<folder_name>\n",
        "COLAB_DATA_PATH = \"/content/drive/MyDrive/Medhack_data\"\n",
        "\n",
        "def get_data_dir():\n",
        "    \"\"\"Load data from Google Drive. Returns Path to folder containing the lagged CSV files.\"\"\"\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "        data_dir = Path(COLAB_DATA_PATH)\n",
        "        if not (data_dir / \"train_data_lagged.csv\").exists():\n",
        "            raise FileNotFoundError(\n",
        "                f\"train_data_lagged.csv not found in {data_dir}. \"\n",
        "                f\"Update COLAB_DATA_PATH to match your Drive folder.\"\n",
        "            )\n",
        "        return data_dir\n",
        "    except ImportError:\n",
        "        # Not in Colab: download via gdown\n",
        "        output_dir = Path(\"data_from_drive\")\n",
        "        output_dir.mkdir(exist_ok=True)\n",
        "        gdown.download_folder(url=GOOGLE_DRIVE_URL, output=str(output_dir), quiet=False)\n",
        "        if (output_dir / \"train_data_lagged.csv\").exists():\n",
        "            return output_dir\n",
        "        for sub in output_dir.iterdir():\n",
        "            if sub.is_dir() and (sub / \"train_data_lagged.csv\").exists():\n",
        "                return sub\n",
        "        raise FileNotFoundError(\n",
        "            f\"train_data_lagged.csv not found in {output_dir}. \"\n",
        "            f\"Ensure the Drive folder contains train_data_lagged.csv, test_data_lagged.csv, holdout_data_lagged.csv.\"\n",
        "        )\n",
        "\n",
        "DATA_DIR = Path(get_data_dir())\n",
        "print(f\"DATA_DIR = {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      9\u001b[39m DATA_DIR = Path(\u001b[33m'\u001b[39m\u001b[33m../data\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m MODELS_DIR = Path(\u001b[33m'\u001b[39m\u001b[33m../models\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import tensorflow as tf\n",
        "\n",
        "# DATA_DIR is set by the \"Load Data from Google Drive\" cell above\n",
        "MODELS_DIR = Path('../models')\n",
        "MODELS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "VITAL_COLS = ['heart_rate', 'systolic_bp', 'diastolic_bp', 'respiratory_rate', 'oxygen_saturation']\n",
        "N_LAGS = 6\n",
        "TRAIN_SAMPLE_SIZE = 500_000  # LSTM can handle more than SVM; increase for full training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Reshape Data\n",
        "\n",
        "Reshape flat lag columns into `(n_samples, timesteps, features)`. Each sample: `[lag6, lag5, lag4, lag3, lag2, lag1, current]` for each of 5 vitals → shape `(7, 5)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reshape_lagged_to_sequences(df: pd.DataFrame, vital_cols: list, n_lags: int) -> np.ndarray:\n",
        "    \"\"\"Reshape lagged columns into (n_samples, timesteps, n_vitals).\n",
        "    \n",
        "    Timestep order: lag_n, ..., lag_1, current (oldest to newest).\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    for col in vital_cols:\n",
        "        # Columns: lag6, lag5, ..., lag1, current (col)\n",
        "        lag_cols = [f'{col}_lag{i}' for i in range(n_lags, 0, -1)]  # lag6..lag1\n",
        "        seq_cols = lag_cols + [col]\n",
        "        sequences.append(df[seq_cols].values)  # (n_samples, 7)\n",
        "    # Stack: (n_samples, 7, 5) - each row is (timesteps, vitals)\n",
        "    X = np.stack(sequences, axis=-1)\n",
        "    return X\n",
        "\n",
        "\n",
        "train = pd.read_csv(DATA_DIR / 'train_data_lagged.csv')\n",
        "test = pd.read_csv(DATA_DIR / 'test_data_lagged.csv')\n",
        "\n",
        "X_train_full = reshape_lagged_to_sequences(train, VITAL_COLS, N_LAGS)\n",
        "y_train_full = train['label'].values\n",
        "X_test = reshape_lagged_to_sequences(test, VITAL_COLS, N_LAGS)\n",
        "y_test = test['label'].values\n",
        "\n",
        "print(f\"Train sequences: {X_train_full.shape}\")  # (n, 7, 5)\n",
        "print(f\"Test sequences: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stratified Sample and Standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "splitter = StratifiedShuffleSplit(n_splits=1, train_size=TRAIN_SAMPLE_SIZE, random_state=42)\n",
        "idx, _ = next(splitter.split(X_train_full, y_train_full))\n",
        "X_train = X_train_full[idx]\n",
        "y_train = y_train_full[idx]\n",
        "\n",
        "print(f\"Sampled train: {X_train.shape}\")\n",
        "print(pd.Series(y_train).value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardize: fit on train, apply to train and test\n",
        "n_samples, n_timesteps, n_features = X_train.shape\n",
        "X_train_flat = X_train.reshape(-1, n_features)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train_flat)\n",
        "\n",
        "X_train_scaled = scaler.transform(X_train_flat).reshape(n_samples, n_timesteps, n_features)\n",
        "X_test_flat = X_test.reshape(-1, n_features)\n",
        "X_test_scaled = scaler.transform(X_test_flat).reshape(X_test.shape[0], n_timesteps, n_features)\n",
        "\n",
        "print(f\"Scaled train: {X_train_scaled.shape}, test: {X_test_scaled.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_lstm_model(n_timesteps: int, n_features: int, n_classes: int = 4, class_weights: dict | None = None):\n",
        "    \"\"\"Unidirectional LSTM for time series classification.\"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(n_timesteps, n_features)),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.LSTM(32),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(n_classes, activation='softmax'),\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "n_classes = len(np.unique(y_train))\n",
        "class_counts = np.bincount(y_train)\n",
        "total = len(y_train)\n",
        "class_weights = {i: total / (n_classes * c) for i, c in enumerate(class_counts)}\n",
        "print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "model = build_lstm_model(n_timesteps, n_features, n_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    X_train_scaled,\n",
        "    y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=10,\n",
        "    batch_size=512,\n",
        "    class_weight=class_weights,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "model.save(MODELS_DIR / 'lstm_baseline.keras')\n",
        "print(f\"Model saved to {MODELS_DIR / 'lstm_baseline.keras'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate ROC AUC on Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_proba = model.predict(X_test_scaled)\n",
        "roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr', average='macro')\n",
        "print(f\"Test ROC AUC (macro, OVR): {roc_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Holdout Predictions (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "holdout = pd.read_csv(DATA_DIR / 'holdout_data_lagged.csv')\n",
        "X_holdout = reshape_lagged_to_sequences(holdout, VITAL_COLS, N_LAGS)\n",
        "X_holdout_flat = X_holdout.reshape(-1, n_features)\n",
        "X_holdout_scaled = scaler.transform(X_holdout_flat).reshape(X_holdout.shape[0], n_timesteps, n_features)\n",
        "\n",
        "y_holdout_proba = model.predict(X_holdout_scaled)\n",
        "print(f\"Holdout predictions shape: {y_holdout_proba.shape}\")\n",
        "\n",
        "if 'label' in holdout.columns:\n",
        "    roc_auc_holdout = roc_auc_score(holdout['label'], y_holdout_proba, multi_class='ovr', average='macro')\n",
        "    print(f\"Holdout ROC AUC (macro, OVR): {roc_auc_holdout:.4f}\")\n",
        "else:\n",
        "    print(\"Holdout has no labels; predictions ready for submission.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
